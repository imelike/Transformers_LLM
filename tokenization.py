'''
 **PART B:**  Hugging Face Tokenization: Use a Pre-trained Tokenizer or Train a New Tokenizer from scratch?

- **gpt2-turkish-cased** modelini eitirken kullan覺lm覺 olan tokenizer'覺 kendi modelimizde kullanal覺m.
Bu Tokenizer T羹rk癟e corpus 羹zerinde eitildii i癟in T羹rk癟e'ye uygun tokenlar elde etmemiz daha kolay olur.

- Verilen metni Tokenize etmek i癟in iki y繹ntem var:
 - 1) input string'in ilk n token'覺n覺 al覺r覺z. 繹r. token say覺s覺n覺 n=40 belirlemisek ilk 40 token'覺 al覺r覺z geri kalan c羹mle dikkate al覺nmaz.

 - 2) input string'in t羹m tokenlar覺 al覺n覺r. 繹r. c羹mle 120 tokendan oluuyorsa bundan 3 tane 40'l覺k sequence elde ederiz.

 !!! Haz覺rlayaca覺m dil modelini metin 羹retme(generate) i癟in kullanaca覺m ve metnin T羹rk癟e gramer kurallar覺na(noktalama iaretleri) uymas覺 i癟in 1. y繹ntemi kullanaca覺m.

 (tehlikeli!!) 2. y繹ntemde c羹mlenin ortas覺ndan balayaca覺 ek. model eitilebilir.
'''

import os
from datasets import load_dataset


# Tokenize edeceim temizlediim, haz覺r veri setini Hugging Face Hub'dan indireceim
# bu veri setini prepare.py de temizlemitim
reviews_sample = load_dataset("imelike/turkishReviews-ds-mini")
reviews_sample


from transformers import AutoTokenizer

context_length = 40

# Hugging Face'teki yukar覺da dediimiz modelin Tokenizer'覺n覺 kullan覺yoruz
pretrained_tokenizer = AutoTokenizer.from_pretrained("redrussianarmy/gpt2-turkish-cased")

outputs = pretrained_tokenizer(
    # imelike/turkishReviews-ds-mini train data'm覺zdaki review'a uyguluyoruz
    reviews_sample["train"][:2]["review"],
    truncation=True,  #belirlenen n tokendan sonras覺n覺 alma
    max_length=context_length,
    return_overflowing_tokens=False, ##belirlenen n tokendan sonras覺n覺 yeni bir sequence e ekleme
    return_length=True, #ka癟l覺k token serisi 羹rettii
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")

# redrussianarmy/gpt2-turkish-cased modelindeki tokenizer s繹zl羹羹
# 50258 kelime var
print("vocab_size: ", len(pretrained_tokenizer))

# kendi veri setimizdeki bir yorumun ald覺覺m覺z modelin tokenizer'覺nda kar覺l覺k geldii kelimelerin numaralar覺
txt = "S羹rat Kargom Hala Gelmedi,1402 numaral覺 kargom adatepe ubesinde."
tokens = pretrained_tokenizer(txt)['input_ids']
print(tokens)


'''
**NOTE(TR)**:

- yukar覺daki say覺lar覺n, redrussianarmy/gpt2-turkish-cased tokenizer s繹zl羹羹nde kar覺l覺k geldii kelimeler
- S羹rat ==> s羹r + at,  Kargom==> kar + g + om olarak alm覺

 GPT-2'de kullan覺lan Tokenizer():
-  kelime bazl覺 da olabilir
-  subword(heceleme) de olabilir
-  harf bazl覺 da olabilir bu 羹癟 farkl覺 ek. oalbilir

 
- burada genellikle kullan覺lan subword par癟alama yap覺lm覺, modelimiz bunu 繹renebilir fakat
- bizim veri setimize bu haz覺r pretrained-token uygun deil kendi modelinde baar覺l覺 olabilir ama burada anlaml覺 tokenlar 羹retemedi
- bunun sebebi t羹rk癟e sondan eklemeli bir dil, kendi veri setimdeki kelimelerin 癟ok bozuk olduunu g繹rd羹
- Bir model i癟inde b羹t羹n kelimeleri b繹yle subword 繹renmek zor o y羹zden,
- 癟ok buyuk T羹rk癟e veri setinde eitilmemi tokenlar hari癟 kendi Tokenizer()覺m覺z kendimiz eitmemiz,
- modelin baar覺s覺 i癟in daha iyi olur
'''

converted = pretrained_tokenizer.convert_ids_to_tokens(tokens)
print(converted)



###### TRAIN A NEW TOKENIZER ######

# Bu fonksiyon list comprehension yap覺yor
# List comprehension her bir batch_size kadar覺n覺 bizim train datam覺zdan alacak, bize par癟a par癟a d繹nd羹recek
# training_corpus bizim i癟in art覺k data generator olcak
def get_training_corpus():
    batch_size = 1000
    return (
        reviews_sample["train"][i : i + batch_size]["review"]
        for i in range(0, len(reviews_sample["train"]), batch_size)
    )
training_corpus = get_training_corpus()


# 1 tane review ald覺覺mda 1000 tane review geliyor 癟羹nk羹 batch_size'覺 1000 yapt覺m yukarda
# toplamda 3378 tane gedi train'de bu kadar veri vard覺 癟羹nk羹 geri kalan覺 validation'd覺
for reviews in get_training_corpus():
    print(len(reviews))

'''
# imdi Tokenizer()'覺 yaratac覺z bu Tokenizer()'覺 Hugging Face ile entegre edeceim i癟in hem modeliyle hem data setiyle,
# bu y羹zden Hugging Face'in kurallar覺na g繹re Tokenizer() yarataca覺z
# Hugging Face'te her eyi standart API'de kullaca覺m i癟in Keras, Tensorflow veya Python'da deil Hugging Face'in Tokenizer'覺n覺 kullanaca覺m

# bizim veri seti ilk 4000 reviewdan oluuyor bunun i癟in 52000 vocab_size 癟ok ama ger癟ek uygulamalarda bu vocab_size iyi

# s覺f覺rdan Tokenizer yarat覺rken bir癟ok yapmam gereken ayar var(kullaca覺m model ile ilgili)
# bu ayarlar i癟in, GPT-2'de 癟al覺t覺覺n覺 bildiim ve indirdiim pretrained_tokenizer'覺n ayarlar覺n覺 kullanaca覺m
# train_new_from_iterator: ona verdiim data generator'覺 kullanarak istedii vocab_size'a g繹re, benim elimdeki tokenizer'覺n konfig羹rasyonuna(pretrained_tokenize) g繹re yeniden Tokenizer yaratacak
# bunun g羹zellii biz bu Tokenizer'覺 GPT-2'de kullanaca覺z ve orada subword ayarlar覺 var bunlar覺 yapmaktan kurtulmu oluyorum

# vocab size: token say覺s覺 demek
# !!!! bir kelime birden fazla token'a b繹l羹nebilir

vocab_size = 52000
tokenizer = pretrained_tokenizer.train_new_from_iterator(training_corpus,vocab_size)
'''

vocab_size = 52000
tokenizer = pretrained_tokenizer.train_new_from_iterator(training_corpus,vocab_size)

# end of string token, bunun s覺f覺r olduuna emin olmam覺z laz覺m
tokenizer.eos_token_id

# 44208 token elde edilmi (ilk 4000 review i癟in)
tokenizer.vocab_size

# yeni Tokenizer'覺m覺z覺 kullanarak ayn覺 metinde deneyelim
txt = "S羹rat Kargom Hala Gelmedi,1402 numaral覺 kargom adatepe ubesinde."
tokens = tokenizer(txt)['input_ids']
print(tokens)

converted = tokenizer.convert_ids_to_tokens(tokens)
print(converted)

# pretrained_tokenizer ayn覺 metni 19 token la g繹sterirken
# benim tokenizer'覺m ayn覺 metni 12 tokenla g繹sterir,
# b繹ylece uygun s覺rada c羹mle 羹retmek daha kolay
print(len(tokenizer.tokenize(txt)))
print(len(pretrained_tokenizer.tokenize(txt)))


###### SAVING THE TOKENIZER ######

# Kendi Tokenizer'覺m覺 eittikten sonra localde saklayaca覺m
#path="./"
#file_name="turkishReviews-ds-mini"
#tokenizer.save_pretrained(path+file_name)

# Tokenizer'覺m覺z覺 localden indirelim
loaded_tokenizer = AutoTokenizer.from_pretrained("./turkishReviews-ds-mini")

# Kendi Tokenizer'覺m覺 eittikten sonra Hugging Face Hub'a y羹kleyelim
# huggingface-cli login kodunu terminale yaz
os.environ["TOKENIZERS_PARALLELISM"] = "false"
tokenizer.push_to_hub("imelike/turkishReviews-ds-mini")

# kendi 羹rettiimiz Tokenizer()'覺 Hugging Face Hub'dan indirelim
downloaded_tokenizer = AutoTokenizer.from_pretrained("imelike/turkishReviews-ds-mini")


# tokenizer'覺m覺z覺n doru inip-inmediini kontrol edelim
# 羹癟羹 de ayn覺ysa doru 癟al覺覺yor
txt = "S羹rat Kargom Hala Gelmedi,1402 numaral覺 kargom adatepe ubesinde."
tokens = tokenizer(txt)['input_ids']
print("trained tokenizer:", tokens)
tokens = loaded_tokenizer(txt)['input_ids']   #locale y羹klenen
print("loaded tokenizer:", tokens)
tokens = downloaded_tokenizer(txt)['input_ids']
print("downloaded tokenizer:", tokens)

